{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic with Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay database implemented using a sum tree. The priority of a transition is based on its TD-error, which tells how good than expected (estemiated by critic network) is this transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](res/ACERP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "from gym import wrappers\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from Network.deepnetsoftmax import Netsoftmax\n",
    "from Network.deepnet import Net\n",
    "from Memory.MemoryBuffer import ReplayMemory\n",
    "\n",
    "from Agents import Actor, Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Tree Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \n",
    "    def __init__ (self, capacity):\n",
    "        \n",
    "    \n",
    "    def add(self, p, data): \n",
    "    # add node\n",
    "       \n",
    "        \n",
    "        \n",
    "    def update(self, tree_idx, p):\n",
    "    # update a node with new key (td-error)\n",
    "        \n",
    "    def sample(self, v):\n",
    "    # prioritized sample from memoory\n",
    "        \n",
    "    @property\n",
    "    def total_p(self):\n",
    "    # get sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumTreeMemoryBuffer(object):\n",
    "    \n",
    "    def __init__():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "def start_r(GAME_NAME, BATCH_SIZE, MEMORY_CAPACITY):\n",
    "    print \"make enviornment\"\n",
    "    env = gym.make(GAME_NAME)\n",
    "    print \"create actor, critic\"\n",
    "    actor = Actor(env.observation_space, env.action_space)\n",
    "    critic = Critic(env.observation_space, env.action_space)\n",
    "    reward_per_epi=[]\n",
    "    durations_per_epi=[]\n",
    "    l_A=[]\n",
    "    l_C=[]\n",
    "    \n",
    "    MAX_EPISODE = 150\n",
    "    RENDER = False\n",
    "    MAX_EP_STEPS= 1000\n",
    "    DISPLAY_REWARD_THRESHOLD=200\n",
    "    BATCH_SIZE=BATCH_SIZE   \n",
    "    MEMORY_CAPACITY=MEMORY_CAPACITY  \n",
    "    replay_memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "    \n",
    "    print \"begin.\\n\\n\"\n",
    "    for i_episode in range(MAX_EPISODE):\n",
    "        s = env.reset()\n",
    "        track_r = []     \n",
    "        critic._v_=[]\n",
    "        actor._loss_=[]\n",
    "        for t in count():\n",
    "            if RENDER: env.render()\n",
    "\n",
    "            a = actor.choose_action(s)\n",
    "\n",
    "            s_, r, done, info = env.step(a)\n",
    "\n",
    "            if done: r = -20    # 回合结束的惩罚/ Penalty if die\n",
    "\n",
    "            track_r.append(r)\n",
    "            \n",
    "            # ACER: Critic Actor with Experience Replay\n",
    "            ################## ################  \n",
    "            if not done:\n",
    "                replay_memory.save(s, a, r, s_)   # Save non-final transition into memeory\n",
    "            \n",
    "            if len(replay_memory) >= BATCH_SIZE:\n",
    "                \n",
    "                transitions = replay_memory.sample(BATCH_SIZE)   # Sample from memory for training\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                \n",
    "                s_b = np.asarray(batch.state)       \n",
    "                s_b_n = np.asarray(batch.next_state)  \n",
    "                a_b = np.asarray(batch.action).reshape(BATCH_SIZE, 1)\n",
    "                r_b = np.asarray(batch.reward).reshape(BATCH_SIZE, 1) \n",
    "                \n",
    "                td_error = critic.learn(s_b, r_b, s_b_n)    # Critic Learn  \n",
    "                actor.learn(s_b, a_b, td_error)       # Actor Learn     \n",
    "            ################## ################\n",
    "\n",
    "            s = s_\n",
    "            \n",
    "            print \"... in episode (%d) step (%d)\" % (i_episode+1,t)\n",
    "            if is_ipython:\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "            #env.render()\n",
    "\n",
    "            if done or t >= MAX_EP_STEPS:\n",
    "                ep_rs_sum = sum(track_r)/float(t)\n",
    "                if 'running_reward' not in globals():\n",
    "                    running_reward = ep_rs_sum\n",
    "                else:\n",
    "                    running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "                if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "                reward_per_epi.append(running_reward)\n",
    "                durations_per_epi.append(t)\n",
    "                l_A.append(np.mean(actor._loss_))\n",
    "                l_C.append(np.mean(critic._loss_))\n",
    "                print(\"episode:\", i_episode, \"  reward:\", running_reward)\n",
    "                plot(reward_per_epi, durations_per_epi, l_A, l_C)\n",
    "\n",
    "                break\n",
    "            \n",
    "    return reward_per_epi, durations_per_epi, l_A, l_C       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
